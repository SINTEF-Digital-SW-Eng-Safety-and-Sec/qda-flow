{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affordance-Based Interview Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates a Python 3.10 pipeline for analyzing interview text and extracting relationships among people (with roles) and technologies, based on the affordance theory from Melville et al. (2023). We:\n",
    "\n",
    "1. Show conda environment setup instructions.\n",
    "2. Ingest mock text (can be replaced with real `.docx` or `.pdf` parsing).\n",
    "3. Perform Named Entity Recognition (NER) and simple role assignment.\n",
    "4. Detect interactions in sentences, tagging them with **Expansive Decision Making (Hdm)**, **Creativity Automation (Hca)**, **Relationship with Humans (Hrh)**, or **Intermachine Teaming (Hmt)**.\n",
    "5. Analyze sentiment for each interaction.\n",
    "6. Build and visualize a NetworkX graph of these relationships.\n",
    "7. Export nodes and edges to CSV for Gephi.\n",
    "\n",
    "## 1. Conda Environment Setup\n",
    "```bash\n",
    "conda create -n interview_analysis python=3.10 -y\n",
    "conda activate interview_analysis\n",
    "\n",
    "pip install spacy==3.5.1 networkx==3.1 textblob==0.17.1 python-docx==0.8.11 pdfplumber==0.9.0\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "## 2. Example Use\n",
    "Run the cells below. In a real application, you'd parse `.docx`/`.pdf` documents from a folder (using `docx2txt` or `pdfplumber`). Here, we just define inlined mock text to simulate an interview.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {},
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load spaCy (small English model)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"SpaCy and libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mock Interview Data\n",
    "Below we define two short interview-like texts in Python variables. A real pipeline would parse from `.docx` or `.pdf` instead. We'll show how one might handle that by reading the text into a dictionary or list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Mock interview text #1\n",
    "interview1 = '''\n",
    "Alice is a data scientist. She works at AcmeCorp. She uses the MLAnalytics system for predictive modeling.\n",
    \"'She found it helpful for making decisions about future trends. Sometimes it even generates new ideas for marketing.\\n\\n\",
    \"Bob, a project manager, also relies on MLAnalytics system for decisions. He says it's vital. \",
    \"Bob also introduced a Virtual Assistant Tool, which has a friendly voice, to help employees schedule meetings.\\n\"\"\"",
    "\n",
    "# Mock interview text #2\n",
    "interview2 = '''\n",
    "Carol is a customer of AcmeCorp. She tries the Virtual Assistant Tool daily.\\n",
    \"It personalizes her recommendations and interacts with her frequently. There's also a second AI system called RecommenderX software, \",
    \"which shares data with Virtual Assistant Tool. \",
    \"Carol says sometimes the recommendations are confusing.\\n\"\"\"",
    "\n",
    "# Put these into a dictionary simulating file-based reading\n",
    "documents = {\n",
    "    \"Interview1.txt\": interview1,\n",
    "    \"Interview2.txt\": interview2,\n",
    "}\n",
    "print(\"Mock documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity Extraction & Role Assignment\n",
    "We'll:\n",
    "1. Extract **Person** entities with spaCy.\n",
    "2. Try to detect **technology** mentions (\"Tool\", \"system\", \"software\", etc.) with simple regex.\n",
    "3. Assign roles to persons using keywords (\"developer\", \"scientist\", \"manager\", \"user\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We'll create a global store of unique entities across all docs.\n",
    "# Each entity will be { name: {\"type\": <Person|Technology>, \"role\": <role or None> } }\n",
    "\n",
    "all_entities = {}\n",
    "tech_keywords = [\"system\", \"software\", \"tool\", \"platform\"]\n",
    "\n",
    "# Basic role map\n",
    "role_map = {\n",
    "    \"Developer\": [\"developer\", \"engineer\", \"programmer\", \"coder\", \"scientist\"],\n",
    "    \"Manager\": [\"manager\", \"lead\", \"director\", \"chief\", \"project manager\"],\n",
    "    \"User\": [\"user\", \"customer\", \"client\", \"end-user\", \"employee\"]\n",
    "}\n",
    "\n",
    "def extract_entities_and_roles(text, entity_store):\n",
    "    \"\"\"Extract person and technology entities from text, assign roles if found.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # 1) Extract Person entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            name = ent.text.strip()\n",
    "            if name not in entity_store:\n",
    "                entity_store[name] = {\"type\": \"Person\", \"role\": None}\n",
    "\n",
    "    # 2) Heuristic for technology mentions\n",
    "    # We'll do a simple search for capitalized tokens followed by a tech keyword.\n",
    "    tokens = text.split()\n",
    "    for i in range(len(tokens)-1):\n",
    "        t1 = tokens[i].strip(\".,!?\\n\")\n",
    "        t2 = tokens[i+1].strip(\".,!?\\n\").lower()\n",
    "        if t2 in tech_keywords:\n",
    "            # form the tech name\n",
    "            tech_name = t1 + \" \" + t2.capitalize()\n",
    "            if tech_name not in entity_store:\n",
    "                entity_store[tech_name] = {\"type\": \"Technology\", \"role\": None}\n",
    "\n",
    "    # also catch references to 'software' or 'system' after custom names, e.g. \"RecommenderX software\"\n",
    "    # We'll do a regex pass for pattern: (Word+) <tech_keyword>\n",
    "    pattern = rf\"\\b([A-Z][A-Za-z0-9]+(?:[A-Za-z0-9]+)*) (?:{'|'.join(tech_keywords)})\\b\"\n",
    "    matches = re.finditer(pattern, text)\n",
    "    for m in matches:\n",
    "        # e.g. 'RecommenderX software'\n",
    "        t_name = m.group(0)\n",
    "        if t_name not in entity_store:\n",
    "            entity_store[t_name] = {\"type\": \"Technology\", \"role\": None}\n",
    "\n",
    "    # 3) Assign roles by searching for patterns like \"Name is a <something>\" or \"Name, a <something>,\"\n",
    "    for name, info in entity_store.items():\n",
    "        if info[\"type\"] != \"Person\":\n",
    "            continue\n",
    "        # simple patterns\n",
    "        # e.g. \"Alice is a data scientist.\" or \"Bob, a project manager,\" etc.\n",
    "        # We'll do a case-insensitive search.\n",
    "        pattern1 = rf\"{name} is a ([^.]+)\\.\"\n",
    "        pattern2 = rf\"{name}, a ([^,]+),\"\n",
    "\n",
    "        found_role = None\n",
    "        # search pattern1 in text\n",
    "        m1 = re.search(pattern1, text, flags=re.IGNORECASE)\n",
    "        if m1:\n",
    "            desc = m1.group(1).lower()\n",
    "            for rcat, kwlist in role_map.items():\n",
    "                if any(kw in desc for kw in kwlist):\n",
    "                    found_role = rcat\n",
    "                    break\n",
    "        # search pattern2\n",
    "        if not found_role:\n",
    "            m2 = re.search(pattern2, text, flags=re.IGNORECASE)\n",
    "            if m2:\n",
    "                desc = m2.group(1).lower()\n",
    "                for rcat, kwlist in role_map.items():\n",
    "                    if any(kw in desc for kw in kwlist):\n",
    "                        found_role = rcat\n",
    "                        break\n",
    "        if found_role:\n",
    "            entity_store[name][\"role\"] = found_role\n",
    "\n",
    "# Process each mock doc\n",
    "for fname, text in documents.items():\n",
    "    extract_entities_and_roles(text, all_entities)\n",
    "\n",
    "print(\"Extracted entities:\")\n",
    "for k,v in all_entities.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Interaction Extraction + Affordance Classification\n",
    "We'll find pairs of entities in each sentence, then classify the relationship into:\n",
    "- **Hdm (Expansive Decision Making)**: keywords like *decision, analysis, predict, recommend*...\n",
    "- **Hca (Creativity Automation)**: keywords like *creative, create, generate, design, idea*...\n",
    "- **Hrh (Relationship with Humans)**: default for Personâ†’Tech usage, or keywords like *help, interact, use*.\n",
    "- **Hmt (Intermachine Teaming)**: if the sentence has two technologies.\n",
    "\n",
    "We'll also compute a sentiment score (TextBlob polarity)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def classify_affordance(sentence, type1, type2):\n",
    "    \"\"\"Classify a pair's affordance type based on their combined types (Person/Tech) and keywords in sentence.\"\"\"\n",
    "    s = sentence.lower()\n",
    "    if type1 == \"Technology\" and type2 == \"Technology\":\n",
    "        return \"Hmt\"  # inter-machine teaming\n",
    "    # otherwise Person<->Tech\n",
    "    # check some keywords\n",
    "    if re.search(r\"\\b(decision|decide|analysis|analys|predict|recommend|suggest|inference)\\b\", s):\n",
    "        return \"Hdm\"\n",
    "    elif re.search(r\"\\b(creative|create|generate|design|idea|art|innovate|write)\\b\", s):\n",
    "        return \"Hca\"\n",
    "    elif re.search(r\"\\b(interact|assist|help|helpful|use|using|personaliz|adapt|experience)\\b\", s):\n",
    "        return \"Hrh\"\n",
    "    else:\n",
    "        # default\n",
    "        return \"Hrh\"\n",
    "\n",
    "interactions = []  # (entity1, entity2, affordance, sentiment)\n",
    "\n",
    "for fname, text in documents.items():\n",
    "    # split to sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    for sent in sentences:\n",
    "        sent_strip = sent.strip()\n",
    "        if not sent_strip:\n",
    "            continue\n",
    "        present_entities = []\n",
    "        # check which known entities appear in this sentence\n",
    "        for e in all_entities.keys():\n",
    "            if re.search(rf\"\\b{re.escape(e)}\\b\", sent, flags=re.IGNORECASE):\n",
    "                present_entities.append(e)\n",
    "\n",
    "        if len(present_entities) < 2:\n",
    "            continue\n",
    "        # sentiment\n",
    "        polarity = TextBlob(sent_strip).sentiment.polarity  # -1 to +1\n",
    "\n",
    "        # consider all pairs\n",
    "        for i in range(len(present_entities)):\n",
    "            for j in range(i+1, len(present_entities)):\n",
    "                ent1 = present_entities[i]\n",
    "                ent2 = present_entities[j]\n",
    "                type1 = all_entities[ent1][\"type\"]\n",
    "                type2 = all_entities[ent2][\"type\"]\n",
    "                if type1 == \"Person\" and type2 == \"Person\":\n",
    "                    continue  # skip person-person for now\n",
    "                if not (type1 == \"Technology\" or type2 == \"Technology\") and not (type1 == \"Technology\" and type2 == \"Technology\"):\n",
    "                    continue\n",
    "                # classify\n",
    "                aff = classify_affordance(sent_strip, type1, type2)\n",
    "                # store\n",
    "                interactions.append((ent1, ent2, aff, polarity))\n",
    "\n",
    "print(\"Detected interactions:\")\n",
    "for i in interactions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the NetworkX Graph and Visualize\n",
    "We'll create a graph with nodes for each entity, edges for each interaction. We'll store attributes like `entity_type`, `role` (for people), `affordance`, and `sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (with attributes)\n",
    "for name, info in all_entities.items():\n",
    "    G.add_node(name, entity_type=info[\"type\"], role=info.get(\"role\", \"\"))\n",
    "\n",
    "# Add edges (avoid duplicates if multiple mentions of same pair)\n",
    "seen_edges = set()\n",
    "for (ent1, ent2, aff, pol) in interactions:\n",
    "    # sort them for undirected consistency\n",
    "    edge_nodes = tuple(sorted([ent1, ent2]))\n",
    "    if edge_nodes not in seen_edges:\n",
    "        # create the edge\n",
    "        G.add_edge(edge_nodes[0], edge_nodes[1], affordance=aff, sentiment=pol)\n",
    "        seen_edges.add(edge_nodes)\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Basic visualization with matplotlib\n",
    "color_map = []\n",
    "for node, data in G.nodes(data=True):\n",
    "    if data[\"entity_type\"] == \"Person\":\n",
    "        color_map.append(\"skyblue\")\n",
    "    else:\n",
    "        color_map.append(\"orange\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_color=color_map, node_size=1000)\n",
    "\n",
    "# Labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "# Edges\n",
    "nx.draw_networkx_edges(G, pos)\n",
    "\n",
    "# Edge labels - we show only the affordance type\n",
    "edge_labels = nx.get_edge_attributes(G, \"affordance\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"green\")\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(\"Affordance Graph Visualization\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Nodes/Edges to CSV for Gephi\n",
    "We'll create two CSV files:\n",
    "- **nodes.csv** with columns [Id, Label, Type, Role].\n",
    "- **edges.csv** with columns [Source, Target, Type, Affordance, Sentiment].\n",
    "Gephi can import these two CSVs (nodes first, edges second) to reconstruct the graph."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nodes written to nodes.csv\n",
       "Edges written to edges.csv"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export to CSV\n",
    "with open(\"nodes.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Id\", \"Label\", \"Type\", \"Role\"])\n",
    "    for node, data in G.nodes(data=True):\n",
    "        writer.writerow([node, node, data[\"entity_type\"], data.get(\"role\",\"\")])\n",
    "print(\"Nodes written to nodes.csv\")\n",
    "\n",
    "with open(\"edges.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Source\", \"Target\", \"Type\", \"Affordance\", \"Sentiment\"])\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # For undirected edges, we set Type=Undirected for Gephi\n",
    "        writer.writerow([u, v, \"Undirected\", data.get(\"affordance\", \"\"), data.get(\"sentiment\", \"\")])\n",
    "print(\"Edges written to edges.csv\")\n",
    "\n",
    "# Display them as a demonstration in the notebook\n",
    "with open(\"nodes.csv\", \"r\") as f:\n",
    "    print(f.read())\n",
    "with open(\"edges.csv\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "1. **Adapt to real data**: Instead of our mock text, parse your own interview `.docx` or `.pdf` files using `python-docx` (for `.docx`) or `pdfplumber` (for `.pdf`).\n",
    "2. **Improve entity/tech detection**: Our simple heuristic for technology mentions can be replaced with a more robust approach (custom spaCy model or rules).\n",
    "3. **Advanced coref**: We used no real coreference resolution here. Tools like HuggingFace's \"coref\" or spaCy's coref pipeline can help.\n",
    "4. **Refine affordance classification**: Add more nuanced patterns or a machine learning approach to detect expansions in decision-making vs. creativity.\n",
    "5. **Use advanced sentiment**: TextBlob is basic. For more accuracy, try a transformer-based sentiment model.\n",
    "\n",
    "## End of Notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
