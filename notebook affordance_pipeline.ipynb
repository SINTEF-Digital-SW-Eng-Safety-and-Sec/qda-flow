{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affordance-Based Interview Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates a Python 3.10 pipeline for analyzing interview text and extracting relationships among people (with roles) and technologies, based on the affordance theory from Melville et al. (2023). We:\n",
    "\n",
    "1. Show conda environment setup instructions.\n",
    "2. Ingest mock text (can be replaced with real `.docx` or `.pdf` parsing).\n",
    "3. Perform Named Entity Recognition (NER) and simple role assignment.\n",
    "4. Detect interactions in sentences, tagging them with **Expansive Decision Making (Hdm)**, **Creativity Automation (Hca)**, **Relationship with Humans (Hrh)**, or **Intermachine Teaming (Hmt)**.\n",
    "5. Analyze sentiment for each interaction.\n",
    "6. Build and visualize a NetworkX graph of these relationships.\n",
    "7. Export nodes and edges to CSV for Gephi.\n",
    "\n",
    "## 1. Conda Environment Setup\n",
    "```bash\n",
    "conda create -n interview_analysis python=3.10 -y\n",
    "conda activate interview_analysis\n",
    "\n",
    "pip install spacy==3.5.1 networkx==3.1 textblob==0.17.1 python-docx==0.8.11 pdfplumber==0.9.0\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "Once installed, you can open this notebook in that environment and run the cells below.\n",
    "\n",
    "## 2. Example Usage\n",
    "We simulate interview text with inline strings. In practice, you'd parse `.docx`/`.pdf` files from a folder (using `python-docx` or `pdfplumber`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load spaCy (small English model)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"SpaCy and required libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mock Interview Data\n",
    "Below we define two short interview-like texts in Python variables. A real pipeline would parse from `.docx` or `.pdf` files instead. We'll show how one might handle that by reading text into a dictionary or list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Mock interview text #1\n",
    "interview1 = '''\n",
    "Alice is a data scientist. She works at AcmeCorp. She uses the MLAnalytics system for predictive modeling.\n",
    \"She found it helpful for making decisions about future trends. Sometimes it even generates new ideas for marketing.\\n\\n\",
    \"Bob, a project manager, also relies on MLAnalytics system for decisions. He says it's vital.\\n\",
    \"Bob also introduced a Virtual Assistant Tool, which has a friendly voice, to help employees schedule meetings.\n",
    "'''\n",
    "\n",
    "# Mock interview text #2\n",
    "interview2 = '''\n",
    "Carol is a customer of AcmeCorp. She tries the Virtual Assistant Tool daily.\\n",
    \"It personalizes her recommendations and interacts with her frequently. There's also a second AI system called RecommenderX software,\\n\",
    \"which shares data with Virtual Assistant Tool. Carol says sometimes the recommendations are confusing.\n",
    "'''\n",
    "\n",
    "# Put these into a dictionary simulating file-based reading\n",
    "documents = {\n",
    "    \"Interview1.txt\": interview1,\n",
    "    \"Interview2.txt\": interview2,\n",
    "}\n",
    "print(\"Mock documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity Extraction & Role Assignment\n",
    "We'll:\n",
    "1. Extract **Person** entities with spaCy.\n",
    "2. Detect **technology** mentions (\"system\", \"software\", \"tool\", \"platform\", etc.) with regex.\n",
    "3. Assign roles to persons using keywords (\"developer\", \"scientist\", \"manager\", \"user\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_entities = {}  # store unique entities across docs\n",
    "tech_keywords = [\"system\", \"software\", \"tool\", \"platform\"]\n",
    "\n",
    "# Basic role map\n",
    "role_map = {\n",
    "    \"Developer\": [\"developer\", \"engineer\", \"programmer\", \"coder\", \"scientist\"],\n",
    "    \"Manager\": [\"manager\", \"lead\", \"director\", \"chief\", \"project manager\"],\n",
    "    \"User\": [\"user\", \"customer\", \"client\", \"end-user\", \"employee\"]\n",
    "}\n",
    "\n",
    "def extract_entities_and_roles(text, entity_store):\n",
    "    \"\"\"Extract person and technology entities from text, assign roles if found.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # 1) Extract Person entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            name = ent.text.strip()\n",
    "            if name not in entity_store:\n",
    "                entity_store[name] = {\"type\": \"Person\", \"role\": None}\n",
    "\n",
    "    # 2) Heuristic for technology mentions\n",
    "    tokens = text.split()\n",
    "    for i in range(len(tokens)-1):\n",
    "        t1 = tokens[i].strip(\".,!?\\n\")\n",
    "        t2 = tokens[i+1].strip(\".,!?\\n\").lower()\n",
    "        if t2 in tech_keywords:\n",
    "            tech_name = t1 + \" \" + t2.capitalize()  # e.g. 'MLAnalytics system' => 'MLAnalytics System'\n",
    "            if tech_name not in entity_store:\n",
    "                entity_store[tech_name] = {\"type\": \"Technology\", \"role\": None}\n",
    "\n",
    "    # also catch pattern: \"NameSomething software\" (like 'RecommenderX software')\n",
    "    pattern = rf\"\\b([A-Z][A-Za-z0-9]+) (?:{'|'.join(tech_keywords)})\\b\"\n",
    "    for m in re.finditer(pattern, text):\n",
    "        t_name = m.group(0)\n",
    "        if t_name not in entity_store:\n",
    "            entity_store[t_name] = {\"type\": \"Technology\", \"role\": None}\n",
    "\n",
    "    # 3) Assign roles by searching patterns \"Name is a <something>\" or \"Name, a <something>,\"\n",
    "    for name, info in entity_store.items():\n",
    "        if info[\"type\"] != \"Person\":\n",
    "            continue\n",
    "        pattern1 = rf\"{name} is a ([^.]+)\\.\"\n",
    "        pattern2 = rf\"{name}, a ([^,]+),\"\n",
    "\n",
    "        found_role = None\n",
    "        # search pattern1\n",
    "        m1 = re.search(pattern1, text, flags=re.IGNORECASE)\n",
    "        if m1:\n",
    "            desc = m1.group(1).lower()\n",
    "            for rcat, kwlist in role_map.items():\n",
    "                if any(kw in desc for kw in kwlist):\n",
    "                    found_role = rcat\n",
    "                    break\n",
    "        # search pattern2 if not found yet\n",
    "        if not found_role:\n",
    "            m2 = re.search(pattern2, text, flags=re.IGNORECASE)\n",
    "            if m2:\n",
    "                desc = m2.group(1).lower()\n",
    "                for rcat, kwlist in role_map.items():\n",
    "                    if any(kw in desc for kw in kwlist):\n",
    "                        found_role = rcat\n",
    "                        break\n",
    "        if found_role:\n",
    "            entity_store[name][\"role\"] = found_role\n",
    "\n",
    "# Process each mock doc\n",
    "for fname, text in documents.items():\n",
    "    extract_entities_and_roles(text, all_entities)\n",
    "\n",
    "print(\"Extracted entities:\")\n",
    "for k, v in all_entities.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Interaction Extraction + Affordance Classification\n",
    "We will:\n",
    "1. Split each document into sentences.\n",
    "2. For each sentence, list which entities appear.\n",
    "3. If two or more appear, we interpret that as a relationship.\n",
    "4. Classify the relationship (edge) based on keywords:\n",
    "   - **Hdm (Expansive Decision Making)**: *decision, analysis, predict, etc.*\n",
    "   - **Hca (Creativity Automation)**: *create, generate, idea, etc.*\n",
    "   - **Hrh (Relationship with Humans)**: *help, use, interact, default for personâ†”tech*\n",
    "   - **Hmt (Intermachine Teaming)**: if both entities are technologies\n",
    "5. Compute sentiment via TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def classify_affordance(sentence, type1, type2):\n",
    "    \"\"\"Classify a pair's affordance based on types and keywords in the sentence.\"\"\"\n",
    "    s = sentence.lower()\n",
    "    # If both are technologies\n",
    "    if type1 == \"Technology\" and type2 == \"Technology\":\n",
    "        return \"Hmt\"  # Intermachine Teaming\n",
    "    # If Person<->Tech, check for specific keywords\n",
    "    if re.search(r\"\\b(decision|decide|analysis|analys|predict|recommend|suggest|inference)\\b\", s):\n",
    "        return \"Hdm\"\n",
    "    elif re.search(r\"\\b(create|creativity|creative|generate|design|idea|art|innovate|write)\\b\", s):\n",
    "        return \"Hca\"\n",
    "    elif re.search(r\"\\b(interact|assist|help|helpful|use|using|personaliz|adapt|experience)\\b\", s):\n",
    "        return \"Hrh\"\n",
    "    else:\n",
    "        return \"Hrh\"\n",
    "\n",
    "interactions = []  # (entity1, entity2, affordance, polarity)\n",
    "\n",
    "for fname, text in documents.items():\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    for sent in sentences:\n",
    "        sent_strip = sent.strip()\n",
    "        if not sent_strip:\n",
    "            continue\n",
    "        present_entities = []\n",
    "        for e in all_entities.keys():\n",
    "            # if the entity name is found in sentence\n",
    "            if re.search(rf\"\\b{re.escape(e)}\\b\", sent, flags=re.IGNORECASE):\n",
    "                present_entities.append(e)\n",
    "\n",
    "        if len(present_entities) < 2:\n",
    "            continue\n",
    "        polarity = TextBlob(sent_strip).sentiment.polarity\n",
    "\n",
    "        for i in range(len(present_entities)):\n",
    "            for j in range(i+1, len(present_entities)):\n",
    "                ent1 = present_entities[i]\n",
    "                ent2 = present_entities[j]\n",
    "                type1 = all_entities[ent1][\"type\"]\n",
    "                type2 = all_entities[ent2][\"type\"]\n",
    "                # skip person-person edges\n",
    "                if type1 == \"Person\" and type2 == \"Person\":\n",
    "                    continue\n",
    "                # skip if neither is tech\n",
    "                if not (type1 == \"Technology\" or type2 == \"Technology\"):\n",
    "                    continue\n",
    "                # classify\n",
    "                aff = classify_affordance(sent_strip, type1, type2)\n",
    "                interactions.append((ent1, ent2, aff, polarity))\n",
    "\n",
    "print(\"Detected interactions:\")\n",
    "for i in interactions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construct the NetworkX Graph and Visualize\n",
    "We'll create:\n",
    "1. A node for each entity (with attributes: `entity_type`, `role`).\n",
    "2. An edge for each interaction (with attributes: `affordance`, `sentiment`).\n",
    "We then draw a basic visualization using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes\n",
    "for name, info in all_entities.items():\n",
    "    G.add_node(name, entity_type=info[\"type\"], role=info.get(\"role\", \"\"))\n",
    "\n",
    "# Add edges (avoid duplicates if multiple mentions of same pair in the text)\n",
    "seen_pairs = set()\n",
    "for (ent1, ent2, aff, pol) in interactions:\n",
    "    pair = tuple(sorted([ent1, ent2]))\n",
    "    if pair not in seen_pairs:\n",
    "        G.add_edge(pair[0], pair[1], affordance=aff, sentiment=pol)\n",
    "        seen_pairs.add(pair)\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "print(\"Node attributes:\")\n",
    "for n, d in G.nodes(data=True):\n",
    "    print(n, d)\n",
    "\n",
    "print(\"\\nEdge attributes:\")\n",
    "for u, v, d in G.edges(data=True):\n",
    "    print(u, \"-\", v, d)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize with matplotlib\n",
    "color_map = []\n",
    "for node, data in G.nodes(data=True):\n",
    "    if data[\"entity_type\"] == \"Person\":\n",
    "        color_map.append(\"skyblue\")\n",
    "    else:\n",
    "        color_map.append(\"orange\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_color=color_map, node_size=1200)\n",
    "# Labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "# Edges\n",
    "nx.draw_networkx_edges(G, pos)\n",
    "\n",
    "# Edge labels - show only the 'affordance'\n",
    "edge_labels = nx.get_edge_attributes(G, 'affordance')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='green', font_size=8)\n",
    "\n",
    "plt.title(\"Affordance Graph Visualization\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export to CSV for Gephi\n",
    "We'll create two CSV files:\n",
    "- **nodes.csv** with columns [Id,Label,Type,Role].\n",
    "- **edges.csv** with columns [Source,Target,Type,Affordance,Sentiment].\n",
    "Gephi can import these two CSVs (nodes first, edges second) to reconstruct the graph."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nodes written to nodes.csv\n",
       "Edges written to edges.csv"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the graph to CSV files\n",
    "with open(\"nodes.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Id\", \"Label\", \"Type\", \"Role\"])\n",
    "    for node, data in G.nodes(data=True):\n",
    "        writer.writerow([node, node, data[\"entity_type\"], data.get(\"role\", \"\")])\n",
    "\n",
    "print(\"Nodes written to nodes.csv\")\n",
    "\n",
    "with open(\"edges.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Source\", \"Target\", \"Type\", \"Affordance\", \"Sentiment\"])\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        writer.writerow([u, v, \"Undirected\", data.get(\"affordance\", \"\"), data.get(\"sentiment\", \"\")])\n",
    "\n",
    "print(\"Edges written to edges.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. **Adapt to real data**: Instead of our mock text, parse your own `.docx` or `.pdf` interviews using `python-docx` or `pdfplumber`. Store the extracted text in a dict like `documents = {'file1.docx': doc_text, ...}`, then run the same steps.\n",
    "2. **Improve entity/tech detection**: Our approach is simplistic (regex on 'system', 'tool', etc.). A custom spaCy model or more advanced rules can catch domain-specific technologies.\n",
    "3. **Coreference resolution**: Currently we do not handle pronouns thoroughly. Tools like **spacy-coref** or HuggingFace's neural coref models can improve references (e.g., 'she' -> 'Alice').\n",
    "4. **Fine-tune sentiment**: TextBlob is a broad approach. Domain-specific or transformer-based sentiment models could be more precise.\n",
    "5. **Refine affordance detection**: Our keyword-based approach can be replaced by an ML classifier or by more advanced patterns.\n",
    "\n",
    "The final outcome is a graph that can be uploaded to Gephi for advanced visualization and analysis, giving you an overview of how different actors (people) and technologies interact, with edges labeled by the relevant 4IR affordances from Melville et al. (2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Notebook\n",
    "Feel free to adapt and extend this pipeline for your own research and interview data!"
   ]
  }
 ],
 \"metadata\": {
  \"kernelspec\": {
   \"display_name\": \"Python 3.10\",
   \"language\": \"python\",
   \"name\": \"python3\"
  },
  \"language_info\": {
   \"name\": \"python\",
   \"version\": \"3.10\"
  }
 },
 \"nbformat\": 4,
 \"nbformat_minor\": 5
}
